# Training LoRA on Elixirforum Data

```elixir
Mix.install([
  {:bumblebee, "~> 0.4.2"},
  {:axon, "~> 0.6.0"},
  {:nx, "~> 0.6.1"},
  {:exla, "~> 0.6.1"},
  {:explorer, "~> 0.7.0"},
  {:lorax, git: "https://github.com/spawnfest/lorax.git"},
  {:req, "~> 0.4.0"},
  {:kino, "~> 0.11.0"}
])

Nx.default_backend(EXLA.Backend)
```

## Hyperparameters

```elixir
batch_size = 4
sequence_length = 512
r = 8
lora_alpha = 16
lora_dropout = 0.05

:ok
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Load a model

```elixir
{:ok, spec} = Bumblebee.load_spec({:hf, "gpt2"})
{:ok, model} = Bumblebee.load_model({:hf, "gpt2"}, spec: spec)
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "gpt2"})
{:ok, generation_config} = Bumblebee.load_generation_config({:hf, "gpt2"})

:ok
```

<!-- livebook:{"output":true} -->

```
|=============================================================| 100% (548.11 MB)

22:51:53.356 [info] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355

22:51:53.356 [info] XLA service 0x7f5c00016b40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:

22:51:53.356 [info]   StreamExecutor device (0): NVIDIA A10G, Compute Capability 8.6

22:51:53.356 [info] Using BFC allocator.

22:51:53.356 [info] XLA backend allocating 21452488704 bytes on device 0 for BFCAllocator.

22:51:55.530 [info] Loaded cuDNN version 8900

22:51:55.538 [info] Using nvlink for parallel linking
|===============================================================| 100% (1.35 MB)
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Prepare a dataset

```elixir
# text = Kino.Input.textarea("Text Data")
text =
  Req.get!("https://raw.githubusercontent.com/spawnfest/lorax/main/data/elixirforum.txt").body

:ok
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
tokenized_text = %{"input_ids" => input_ids} = Bumblebee.apply_tokenizer(tokenizer, text)
n_tokens = Nx.size(input_ids)
n_train = round(n_tokens * 0.9)
n_val = n_tokens - n_train

train_data =
  for {input_key, tokenized_values} <- tokenized_text, into: %{} do
    {input_key, Nx.slice_along_axis(tokenized_values, 0, n_train, axis: -1)}
  end

test_data =
  for {input_key, tokenized_values} <- tokenized_text, into: %{} do
    {input_key, Nx.slice_along_axis(tokenized_values, n_train, n_val, axis: -1)}
  end

:ok
```

<!-- livebook:{"output":true} -->

```
:ok
```

```elixir
defmodule DataStream do
  def get_batch_stream(%{"input_ids" => input_ids} = data, batch_size, block_size, opts \\ []) do
    seed = Keyword.get(opts, :seed, 1337)

    Stream.resource(
      # initialization function
      fn ->
        Nx.Random.key(seed)
      end,
      # generation function
      fn key ->
        {_b, t} = Nx.shape(input_ids)

        data =
          for {k, v} <- data, into: %{} do
            {k, Nx.reshape(v, {t})}
          end

        # ix = list of random starting indices
        {ix, new_key} =
          Nx.Random.randint(key, 0, t - block_size, shape: {batch_size}, type: :u32)

        ix = Nx.to_list(ix)

        # x is map of sliced tensors
        x =
          for {k, tensor} <- data, into: %{} do
            batch_slice =
              ix
              |> Enum.map(fn i -> Nx.slice_along_axis(tensor, i, block_size, axis: -1) end)
              |> Nx.stack()

            {k, batch_slice}
          end

        # y represents all the predicted next tokens (input_ids shifted by 1) 
        y =
          ix
          |> Enum.map(fn i ->
            data["input_ids"] |> Nx.slice_along_axis(i + 1, block_size, axis: -1)
          end)
          |> Nx.stack()
          |> Nx.flatten()

        out_data = {x, y}

        {[out_data], new_key}
      end,
      fn _ -> :ok end
    )
  end
end
```

<!-- livebook:{"output":true} -->

```
{:module, DataStream, <<70, 79, 82, 49, 0, 0, 16, ...>>, {:get_batch_stream, 4}}
```

You can see what a single batch looks like by grabbing 1 from the stream:

```elixir
train_batch_stream = DataStream.get_batch_stream(train_data, batch_size, sequence_length)
test_batch_stream = DataStream.get_batch_stream(test_data, batch_size, sequence_length)

[{x, y}] = train_batch_stream |> Enum.take(1)
[{x_val, y_val}] = test_batch_stream |> Enum.take(1)

Bumblebee.Tokenizer.decode(tokenizer, x["input_ids"]) |> IO.inspect()
IO.puts("=====")
Bumblebee.Tokenizer.decode(tokenizer, y) |> IO.inspect()

:ok
```

<!-- livebook:{"output":true} -->

```
["Not necessarily, using or on different field with index usually result in query optimizer combine these two different index in bitwise or operation. Usually it’s not faster to do lookup one then the other because it cause two DB request network latency.\n2 Likes\n\n[2 likes]\n\nsodapopcan:\n\nOh interesting. What my my old colleague going on about then? lol. I tried an explain with an OR on the same column and it still gave me a sequence scan, so I guess I don’t understand like I thought I did and I have some more RTFMing to do.\n\n[0 likes]\n\ntravisf:\n\nAh, so you’d recommend implementing get_record/1 when the record is saved/updated and then using that field as the param in the route?\n\n[0 likes]\n\ntravisf:\n\nThis makes sense, but how does that translate into the route? Like if I had a list of Things what would I put in the Routes.live_path/3 as a param?\n\n[0 likes]\n\nbenwilson512:\n\ntravisf:\nLike if I had a list of ThingsCan you elaborate what you mean? Are you trying to have a route that contains many ids / slugs ? Or a list of different links each with its own id / slug ?\n\n[0 likes]\n\ntravisf:\n\nSorry that wasn’t clear, I was just unsure of how your solution would translate into Routes.live_path(@socket, MyAppWeb.ThingsLive.Show, ??) that’s where I was confused if either the id or the slug would work as a param there.\n\n[0 likes]\n\nsodapopcan:\n\nHave a look at Phoenix.Param. You could do something like:\ndefimpl Phoenix.Param, for: YourApp.SomeContext.SomeSchema do def to_param(schema) do Map.get(schema, :slug, schema.id) end\nend\nTo offer some unsolicited advice, I think it’ll be far less of a headache in the future if you make slugs non-nullable fields and auto-generate them based off of some other field as to not force users to specify one. Take it or leave it, of course!\n",
 ".Rendered.to_iodata/1 (phoenix_live_view 0.18.17) lib/phoenix_live_view/engine.ex:153: Phoenix.HTML.Safe.Phoenix.LiveView.Rendered.to_iodata/3 (phoenix 1.7.1) lib/phoenix/controller.ex:1005: anonymous fn/5 in Phoenix.Controller.template_render_to_iodata/4 (telemetry 1.2.1) /home/steven/webdev/elixir/mono_phoenix_v01/deps/telemetry/src/telemetry.erl:321: :telemetry.span/3 (phoenix 1.7.1) lib/phoenix/controller.ex:971: Phoenix.Controller.render_and_send/4 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/controllers/plays_page_controller.ex:1: MonoPhoenixV01Web.PlaysPageController.action/2 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/controllers/plays_page_controller.ex:1: MonoPhoenixV01Web.PlaysPageController.phoenix_controller_pipeline/2 (phoenix 1.7.1) lib/phoenix/router.ex:425: Phoenix.Router.__call__/5 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/endpoint.ex:1: MonoPhoenixV01Web.Endpoint.plug_builder_call/2 (mono_phoenix_v01 0.1.0) lib/plug/debugger.ex:136: MonoPhoenixV01Web.Endpoint.\"call (overridable 3)\"/2 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/endpoint.ex:1: MonoPhoenixV01Web.Endpoint.call/2 (phoenix 1.7.1) lib/phoenix/endpoint/sync_code_reload_plug.ex:22: Phoenix.Endpoint.SyncCodeReload",
 " {:completed, \"true\"} -> from task in query_acc, where: task.completed == true {:completed, \"false\"} -> from task in query_acc, where: task.completed == false _ -> query_acc end end)\nwhich will filter the data we fetched from database and then return that.\nam i wrong in what i said till now?\nNote - i am a noob and have just started learning elixir if i am wrong in my understanding till now please mention why I am wrong and what i should read to understand better, thank for your patience.\n\n[0 likes]\n\negze:\n\nNo worries.\nquery = from(task in Task)\nwill not immediately execute anything. Ecto.Query is just data that knows what to query for. What makes it run against the DB is the Repo.all() which receives the Ecto.Query as argument.\nWhat is happening in the code is, we take the base query query = from(task in Task) and then add more things to it with Enum.reduce. Once we are done, we run it with Repo.all\n\n[3 likes]\n\nkokolegorille:\n\nYou do not hit database until You use Repo…\nUPDATE: as mentionned in the previous post\n\n[1 like]\n\n\nTitle: Conditionally render element attribute in lv 0.17.9\n\nDaAnalyst:\n\nMigrating templates from 0.15.x to 0.17.9.\nAny ideas on how to conditionally render an element attribute?\n\nBefore I could simply do it as shown below, but it doesn’t work anymore:\n my-attribute<% end %>>\n\n[0 likes]\n\nJohnnyCurran:\n\nTry:\nFalsy values (nil, false) will not show my-attribute on the element, true will render my-attribute\n\n[1 like]\n\nDaAnalyst:\n\nThanks, but are you sure it won’t be rendered if falsy? Asking because I have plenty of templates to migrate and a non trivial amount of JS depending on such attributes.\nAlso, is this documented anywhere (haven’t found it in the guides)? If not, it should be.\n\n[0 likes]\n\nJohnnyCurran:\n\nare you sure it",
 " different from the Elixir one, where everything was explained from the very beginning; it feels more something for people who were already using Phoenix and want to learn LiveView specifically.\nI have a hobby project I want to develop with Phoenix, but my main problem is I’ve never worked with databases before. I’m confused about how working with the database locally and then deploying it would work. Because I know that my Postgres database is in the /usr/local/var/postgres folder, which isn’t my project folder. When I define my schema with Ecto and it creates the tables in that dev/local database, how do I then deploy that to production?\nI see that the config/dev.exs file has the database info for the local database, so I suppose I’ll have a separate config for the production database. Does that mean I need to first create and host a database somewhere, then put the database connection info on that prod config somewhere, and then when I deploy Ecto will configure that database for me? But how would this work for future deploys after the first one, when the prod database already has data in it? If I change my schema and do a migration in dev, when/how would that migration happen in prod? It’s very confusing to me.\nI’ve been looking for tutorials/courses on databases to understand things better, but I seem to find stuff aimed at people who want to be DBAs. I don’t want to be an expert, I just want to know the basics to get a small hobbyist project working. Does anyone have course/tutorial/book recommendations for a complete beginner to get started?\n3 Likes\n\n[3 likes]\n\nthelastinuit:\n\nFirst, welcome!!!\nNow, this question is a mother-load! (joking but there is some truth to that).\nWhen I define my schema with Ecto and it creates the tables in that dev/local database, how do I then deploy that to production?\nYou have diff environment configs, as you mentioned. So the config will be there. However, I need to mention that this question involve DevOps tasks (how to release, when, process before releasing and so on). The deployment process to production is its own art. For that and for starters (unless you already did that), refer to Releasing.\nDoes that mean I need to first create"]
=====
" necessarily, using or on different field with index usually result in query optimizer combine these two different index in bitwise or operation. Usually it’s not faster to do lookup one then the other because it cause two DB request network latency.\n2 Likes\n\n[2 likes]\n\nsodapopcan:\n\nOh interesting. What my my old colleague going on about then? lol. I tried an explain with an OR on the same column and it still gave me a sequence scan, so I guess I don’t understand like I thought I did and I have some more RTFMing to do.\n\n[0 likes]\n\ntravisf:\n\nAh, so you’d recommend implementing get_record/1 when the record is saved/updated and then using that field as the param in the route?\n\n[0 likes]\n\ntravisf:\n\nThis makes sense, but how does that translate into the route? Like if I had a list of Things what would I put in the Routes.live_path/3 as a param?\n\n[0 likes]\n\nbenwilson512:\n\ntravisf:\nLike if I had a list of ThingsCan you elaborate what you mean? Are you trying to have a route that contains many ids / slugs ? Or a list of different links each with its own id / slug ?\n\n[0 likes]\n\ntravisf:\n\nSorry that wasn’t clear, I was just unsure of how your solution would translate into Routes.live_path(@socket, MyAppWeb.ThingsLive.Show, ??) that’s where I was confused if either the id or the slug would work as a param there.\n\n[0 likes]\n\nsodapopcan:\n\nHave a look at Phoenix.Param. You could do something like:\ndefimpl Phoenix.Param, for: YourApp.SomeContext.SomeSchema do def to_param(schema) do Map.get(schema, :slug, schema.id) end\nend\nTo offer some unsolicited advice, I think it’ll be far less of a headache in the future if you make slugs non-nullable fields and auto-generate them based off of some other field as to not force users to specify one. Take it or leave it, of course!\n\nRendered.to_iodata/1 (phoenix_live_view 0.18.17) lib/phoenix_live_view/engine.ex:153: Phoenix.HTML.Safe.Phoenix.LiveView.Rendered.to_iodata/3 (phoenix 1.7.1) lib/phoenix/controller.ex:1005: anonymous fn/5 in Phoenix.Controller.template_render_to_iodata/4 (telemetry 1.2.1) /home/steven/webdev/elixir/mono_phoenix_v01/deps/telemetry/src/telemetry.erl:321: :telemetry.span/3 (phoenix 1.7.1) lib/phoenix/controller.ex:971: Phoenix.Controller.render_and_send/4 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/controllers/plays_page_controller.ex:1: MonoPhoenixV01Web.PlaysPageController.action/2 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/controllers/plays_page_controller.ex:1: MonoPhoenixV01Web.PlaysPageController.phoenix_controller_pipeline/2 (phoenix 1.7.1) lib/phoenix/router.ex:425: Phoenix.Router.__call__/5 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/endpoint.ex:1: MonoPhoenixV01Web.Endpoint.plug_builder_call/2 (mono_phoenix_v01 0.1.0) lib/plug/debugger.ex:136: MonoPhoenixV01Web.Endpoint.\"call (overridable 3)\"/2 (mono_phoenix_v01 0.1.0) lib/mono_phoenix_v01_web/endpoint.ex:1: MonoPhoenixV01Web.Endpoint.call/2 (phoenix 1.7.1) lib/phoenix/endpoint/sync_code_reload_plug.ex:22: Phoenix.Endpoint.SyncCodeReloadPlugcompleted, \"true\"} -> from task in query_acc, where: task.completed == true {:completed, \"false\"} -> from task in query_acc, where: task.completed == false _ -> query_acc end end)\nwhich will filter the data we fetched from database and then return that.\nam i wrong in what i said till now?\nNote - i am a noob and have just started learning elixir if i am wrong in my understanding till now please mention why I am wrong and what i should read to understand better, thank for your patience.\n\n[0 likes]\n\negze:\n\nNo worries.\nquery = from(task in Task)\nwill not immediately execute anything. Ecto.Query is just data that knows what to query for. What makes it run against the DB is the Repo.all() which receives the Ecto.Query as argument.\nWhat is happening in the code is, we take the base query query = from(task in Task) and then add more things to it with Enum.reduce. Once we are done, we run it with Repo.all\n\n[3 likes]\n\nkokolegorille:\n\nYou do not hit database until You use Repo…\nUPDATE: as mentionned in the previous post\n\n[1 like]\n\n\nTitle: Conditionally render element at" <> ...
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Train the model

Now we can go about training the model! First, we need to extract the Axon model and parameters from the Bumblebee model map:

```elixir
%{model: model, params: params} = model

model
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"attention_head_mask" => {12, 12}, "attention_mask" => {nil, nil}, "cache" => nil, "input_embeddings" => {nil, nil, 768}, "input_ids" => {nil, nil}, "position_ids" => {nil, nil}}
  outputs: "container_37"
  nodes: 859
>
```

The Axon model actually outputs a map with `:logits`, `:hidden_states`, and `:attentions`. You can see this by using `Axon.get_output_shape/2` with an input. This method symbolically executes the graph and gets the resulting shapes:

```elixir
[{input, _}] = Enum.take(train_batch_stream, 1)
Axon.get_output_shape(model, input)
```

<!-- livebook:{"output":true} -->

```
%{
  cache: #Axon.None<...>,
  logits: {4, 512, 50257},
  cross_attentions: #Axon.None<...>,
  hidden_states: #Axon.None<...>,
  attentions: #Axon.None<...>
}
```

For training LoRA adapters, we'll freeze the original layers, and append adapters to our target nodes

```elixir
lora_model =
  model
  |> Axon.freeze()
  |> Lorax.inject(%Lorax.Config{
    r: r,
    alpha: lora_alpha,
    dropout: lora_dropout,
    target_key: true,
    target_query: true,
    target_value: true
  })
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"attention_head_mask" => {12, 12}, "attention_mask" => {nil, nil}, "cache" => nil, "input_embeddings" => {nil, nil, 768}, "input_ids" => {nil, nil}, "position_ids" => {nil, nil}}
  outputs: "container_37"
  nodes: 895
>
```

Now we can declare our training loop. You can construct Axon training loops using the `Axon.Loop.trainer/3` factory method with a model, loss function, and optimizer. We'll also adjust the log-settings to more frequently log metrics to standard out:

```elixir
defmodule CommonTrain do
  import Nx.Defn

  defn custom_predict_fn(model_predict_fn, params, input) do
    %{prediction: preds} = out = model_predict_fn.(params, input)

    # Output of GPT2 model is a map containing logits and other tensors
    logits = preds.logits

    {b, t, c} = Nx.shape(logits)
    reshaped = Nx.reshape(logits, {b * t, c})
    %{out | prediction: reshaped}
  end

  def custom_loss_fn(y_true, y_pred) do
    Axon.Losses.categorical_cross_entropy(y_true, y_pred,
      from_logits: true,
      sparse: true,
      reduction: :mean
    )
  end
end

{init_fn, predict_fn} = Axon.build(lora_model, mode: :train)
custom_predict_fn = &CommonTrain.custom_predict_fn(predict_fn, &1, &2)
custom_loss_fn = &CommonTrain.custom_loss_fn(&1, &2)

lora_params =
  {init_fn, custom_predict_fn}
  |> Axon.Loop.trainer(custom_loss_fn, Polaris.Optimizers.adam(learning_rate: 3.0e-4))
  |> Axon.Loop.run(train_batch_stream, params, epochs: 1, iterations: 1000, compiler: EXLA)

:ok
```

<!-- livebook:{"output":true} -->

```

22:52:09.408 [debug] Forwarding options: [compiler: EXLA] to JIT compiler

22:52:42.866 [info] ptxas warning : Registers are spilled to local memory in function 'triton_softmax', 1284 bytes spill stores, 1284 bytes spill loads

Epoch: 0, Batch: 950, loss: 3.2910779
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Testing out text Generation

```elixir
lora_model_info = %{model: lora_model, params: lora_params, spec: spec}

lora_generation_config =
  Bumblebee.configure(generation_config,
    max_new_tokens: 512,
    strategy: %{type: :multinomial_sampling, top_p: 0.8}
  )

serving =
  Bumblebee.Text.generation(lora_model_info, tokenizer, lora_generation_config,
    compile: [batch_size: 1, sequence_length: 512],
    stream: true,
    defn_options: [compiler: EXLA, lazy_transfers: :always]
  )

Kino.start_child({Nx.Serving, name: Llama, serving: serving})
```

<!-- livebook:{"output":true} -->

```
{:ok, #PID<0.4468.0>}
```

```elixir
Nx.Serving.batched_run(Llama, "Title: ") |> Enum.each(&IO.write/1)
```

<!-- livebook:{"output":true} -->

```
}
I’m already thinking about deploying a server to the production environment, but if you don’t have the persistence feature set up for the server and want to migrate it to production, just use:
E-mail: phoenix@localhost.params.server { providers: [true] }
Now that I’m set up the server, it’s just like deploying a new app.
And I don’t want to waste time finding a new reason to start writing applications in different configurations.

[0 likes]

erijd:

I can easily start working with an E-mail client. You could simply set up a dashboard and be back in the day and be productive as a lifter.
It’s simpler to say that the first parameter is a new phoenix server or deployment configuration. If you have a server you’ve defined already, you can just add the additional environment variable:
etcd :repository = "DIGITAL-HAS-PS" host = "localhost" server_name = "elixir-server-test" restart_password = "localhost" e-mail_server_prefix = "127.0.0.1" build_origin = "PHP:xenial/msdn/test" configuration = "development"
The script opens a database with your browser and starts your application using the persistent server.
If you don’t know what the server is for, you can add the server:
set db = "server.json" server = "elixir-server-test" client = "perl"

[1 like]


Title: Free documentation is not recommended if there is a requirement for it?

reo-mx:

i don’t recommend the free documentation if there is a requirement for it?


I would not recommend it.

However if you need to test code, there are better resources, like qs://get-documents-test-script.org.

[0 likes]


Title: In client I’ve also tested with one app in different environments

nasitaap:

In client I’ve also tested with one app in different environments

I have used qs://get-documents-test-script.org. I have also used qs://get-documents-test-script
```

<!-- livebook:{"output":true} -->

```
:ok
```

