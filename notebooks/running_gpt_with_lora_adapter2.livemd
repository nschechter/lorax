<!-- livebook:{"persist_outputs":true} -->

# Running GPT2 with LoRA parameters

```elixir
Mix.install([
  {:bumblebee, "~> 0.4.2"},
  {:axon, "~> 0.6.0"},
  {:nx, "~> 0.6.1"},
  {:exla, "~> 0.6.1"},
  {:explorer, "~> 0.7.0"},
  {:lorax, git: "https://github.com/spawnfest/lorax.git"},
  # {:lorax, path: "/Users/ted/CS/elixir/lorax"},
  {:req, "~> 0.4.0"},
  {:kino, "~> 0.11.0"}
])

Nx.default_backend(EXLA.Backend)
```

## Mimicking the Elixirforum Help Section

One great benefit to LoRA is the file size of the fine-tuned parameters. What we can do is load one of the parameters named `elixirforum-help-section.lorax` and upload it to the Kino input down below.

The other notebook in this package fine-tuned the GPT2 model to learn the associations typically found in an Elixirforum thread.

The average computer should be able to run the GPT2 inference without much problems. You'll see that it generates some random thread you may find in the help section.

## Load model

```elixir
{:ok, spec} = Bumblebee.load_spec({:hf, "gpt2"})
{:ok, model} = Bumblebee.load_model({:hf, "gpt2"}, spec: spec)
{:ok, tokenizer} = Bumblebee.load_tokenizer({:hf, "gpt2"})
{:ok, generation_config} = Bumblebee.load_generation_config({:hf, "gpt2"})

%{model: model, params: gpt2_params} = model

:ok
```

<!-- livebook:{"output":true} -->

```

19:49:03.661 [info] TfrtCpuClient created.

```

<!-- livebook:{"output":true} -->

```
:ok
```

## Upload Params

```elixir
input = Kino.Input.file("Lorax Params")
```

## Load Param File

Although we've loaded the LoRA params, we still need the original parameters (the extremely large ones). We'll merge the parameters into one single mapping of layers -> tensor values, and Axon will be able to run it

```elixir
lora_params = Lorax.Params.kino_load_file!(input)
merged_params = Lorax.Params.merge_params(lora_params, gpt2_params)

:ok
```

<!-- livebook:{"output":true} -->

```
:ok
```

## Define LoRA Model

Axon keeps the model definition as code. So we need to modify the definition of the GPT2 layers, so it contains the injected LoRA layers. This is the same definition found in the other notebook that trained the LoRA GPT2 model.

```elixir
r = 8
lora_alpha = 16
lora_dropout = 0.05

lora_model =
  model
  |> Axon.freeze()
  |> Lorax.inject(%Lorax.Config{
    r: r,
    alpha: lora_alpha,
    dropout: lora_dropout,
    target_key: true,
    target_query: true,
    target_value: true
  })
```

<!-- livebook:{"output":true} -->

```
#Axon<
  inputs: %{"attention_head_mask" => {12, 12}, "attention_mask" => {nil, nil}, "cache" => nil, "input_embeddings" => {nil, nil, 768}, "input_ids" => {nil, nil}, "position_ids" => {nil, nil}}
  outputs: "container_37"
  nodes: 895
>
```

## Inference Prepwork

```elixir
lora_model_info = %{model: lora_model, params: merged_params, spec: spec}

lora_generation_config =
  Bumblebee.configure(generation_config,
    max_new_tokens: 512,
    strategy: %{type: :multinomial_sampling, top_p: 0.8}
  )

serving =
  Bumblebee.Text.generation(lora_model_info, tokenizer, lora_generation_config,
    compile: [batch_size: 1, sequence_length: 512],
    stream: true,
    defn_options: [compiler: EXLA, lazy_transfers: :always]
  )

Kino.start_child({Nx.Serving, name: Llama, serving: serving})
```

<!-- livebook:{"output":true} -->

```
{:ok, #PID<0.921.0>}
```

## Inference Results

As far as I can tell, the model doesn't generate any user that actually exists in the Elixirforums. But a real username may pop up if you run it enough times. The training data can be found in the `data` directory. The structure of every thread goes something like

```
Title: ...

Author: bob

<user submitted text>

[number of likes]
```

```elixir
Nx.Serving.batched_run(Llama, "Title: ") |> Enum.each(&IO.write/1)
```

<!-- livebook:{"output":true} -->

```
{"foo"}}

}

[0 likes]

naicmin:

Thanks for the help. I am still having trouble figuring out how to avoid at least one of the hash tags used to infer the response (or cache all pending responses), but the problem appears to be getting much better.
If it’s a valid response, then you should go back and check it out.
If not, please allow me to help you solve it, or maybe I can help you.
I would love to know the exact address of the cache when that gets logged in, or if this was being set up as the root request, etc.

[1 like]

cladd:

What does it mean for phx.h to log in to a new location when they ask for user info?

[1 like]

naicmin:

Thanks for your help! I’m using phx.h in order to query the user database but will need to pass the route to add a parameter to get to the node so the user is there.
If this is a new request, then you can use this to force phx.h to log in as the root, but I don’t have enough time to figure it out (hopefully I will not see that happening in the near future).

[0 likes]


Title: How to search for an exe string in /etc/cache/exe/exe.ex?

keventty:

How to search for an exe string in /etc/cache/exe/exe.ex? Yes
As per your question, which part of this part is used to enter the output and index? The entry is not in the external JSON cache as it was provided by /etc/cache/exe/exe.ex
If the error happens with the default stack error, then look for it in the web exit prompt.

[0 likes]

joevashd:

It is still unclear if this is a caching issue (or just a network issue).

[0 likes]

keventty:

This is just a very basic guideline: get rid of the default stack header and ignore exe string in /etc/cache/exe/exe.ex.
You will not need to fetch the image when /etc/cache/exe/ex
```

<!-- livebook:{"output":true} -->

```
:ok
```
